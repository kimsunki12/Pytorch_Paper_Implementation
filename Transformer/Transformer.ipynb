{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math, copy, time\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "seaborn.set_context(context=\"talk\")\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "QH6oixm3tSNQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scaled Dot-product"
      ],
      "metadata": {
        "id": "8J5RqSZZpxJh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fFUuzcKvpmNq"
      },
      "outputs": [],
      "source": [
        "#Decoder 부분에서는 masking 처리를 해야 하는 부분이 있기에, mask 부분 함께 구현\n",
        "def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
        "  d_k = k.size()[-1] # key 벡터의 차원, scaling factor용\n",
        "  k_transpose = torch.transpose(k, 3, 2)\n",
        "\n",
        "  output = torch.matmul(q, k_transpose) # Attention score 계산\n",
        "  output = output / math.sqrt(d_k) # scale\n",
        "  if mask is not None: # mask 적용할지 말지\n",
        "    output = output.masked_fill(mask.unsqueeze(1).unsqueeze(-1), 0)\n",
        "\n",
        "# softmax로 확률화\n",
        "  output = F.softmax(output, -1)\n",
        "  output = torch.matmul(output, v) # Attention value 계산\n",
        "\n",
        "  return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Multi-Head Attention"
      ],
      "metadata": {
        "id": "cEFvOt-HtFu9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "q, k , v의 입력 모양은 보통 (B, S, D)\n",
        "B: 배치 크기\n",
        "S: 시퀀스 길이\n",
        "D: dim_num, 임베딩 차원"
      ],
      "metadata": {
        "id": "tCl_2AD0tc1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, dim_num=512, head_num=8):\n",
        "        super().__init__()\n",
        "        self.head_num = head_num\n",
        "        self.dim_num = dim_num\n",
        "        # Q, K, V, Output를 위한 Linear 레이어 4개 정의\n",
        "        self.query_embed = nn.Linear(dim_num, dim_num)\n",
        "        self.key_embed = nn.Linear(dim_num, dim_num)\n",
        "        self.value_embed = nn.Linear(dim_num, dim_num)\n",
        "        self.output_embed = nn.Linear(dim_num, dim_num)\n",
        "\n",
        "    #Decoder 부분에서는 masking 처리를 해야 하는 부분이 있기에, mask 부분 함께 구현\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
        "      d_k = k.size()[-1] # key 벡터의 차원, scaling factor용\n",
        "      k_transpose = torch.transpose(k, 3, 2)\n",
        "\n",
        "      output = torch.matmul(q, k_transpose) # 내적으로 Attention score 계산\n",
        "      output = output / math.sqrt(d_k) # scale\n",
        "      if mask is not None: # mask 적용할지 말지\n",
        "        output = output.masked_fill(mask.unsqueeze(1).unsqueeze(-1), 0)\n",
        "\n",
        "    # softmax로 확률화\n",
        "      output = F.softmax(output, -1)\n",
        "      output = torch.matmul(output, v) # Attention value 계산\n",
        "\n",
        "      return output\n",
        "\n",
        "      def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size()[0]\n",
        "\n",
        "        # 헤드 분할\n",
        "        q = self.query_embed(q).view(batch_size, -1, self.head_num, self.d_k).transpose(1,2)\n",
        "        k = self.key_embed(k).view(batch_size, -1, self.head_num, self.d_k).transpose(1,2)\n",
        "        v = self.value_embed(v).view(batch_size, -1, self.head_num, self.d_k).transpose(1,2)\n",
        "        # 어텐션 계산, 병렬적으로 계산\n",
        "        attention_output = self.scaled_dot_product_attention(q, k, v, mask)\n",
        "        # 결과 취합, transpose로 순서 맞추고, view로 물리적으로 이어붙\n",
        "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, -1, self.dim_num)\n",
        "\n",
        "        output = self.output_embed(attention_output)\n",
        "\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "wCg2PdeTsTb8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Residual Add & Layer Norm"
      ],
      "metadata": {
        "id": "TtuJeT6FvklU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AddLayerNorm(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def layer_norm(self, input): #평균을 빼고 표준편차로 나누는 정규화\n",
        "    mean = torch.mean(input, dim=-1, keepdim=True)\n",
        "    std = torch.std(input, dim=-1, keepdim=True)\n",
        "    output = (input-mean) / std\n",
        "    return output\n",
        "\n",
        "  def forward (self, input, residual): #잔차 학\n",
        "    return residual + self.layer_norm(input)"
      ],
      "metadata": {
        "id": "UGgn7LZ0tMab"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Feed Forward"
      ],
      "metadata": {
        "id": "xyUTv5fjzkMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim_num=512):\n",
        "        super().__init__()\n",
        "        #확장 레이어: 입력 차원을 4배로 늘림\n",
        "        self.layer1 = nn.Linear(dim_num, dim_num * 4)\n",
        "        #축소 레이어: 확장된 차원을 다시 원래대로 줄임\n",
        "        self.layer2 = nn.Linear(dim_num * 4, dim_num)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.layer1(input) #확장\n",
        "        output = self.layer2(F.relu(output)) # ReLU 함수 적용 뒤, 축\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "QuJRE6_9zqC3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Encoder"
      ],
      "metadata": {
        "id": "kd-A8TcW0V8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, dim_num=512):\n",
        "        super().__init__()\n",
        "        # 문맥을 정리하는 multi-head attention\n",
        "        self.multihead = MultiHeadAttention(dim_num=dim_num)\n",
        "        # 결과를 정리하는 Add & Norm\n",
        "        self.residual_layer1 = AddLayerNorm()\n",
        "        # 정보를 심화 처리하는 FeedForward\n",
        "        self.feed_forward = FeedForward(dim_num=dim_num)\n",
        "        # 최종 결과를 정리하는 Add & Norm\n",
        "        self.residual_layer2 = AddLayerNorm()\n",
        "\n",
        "    def forward(self, q, k, v):\n",
        "        multihead_output = self.multihead(q, k, v)\n",
        "        residual1_output = self.residual_layer1(multihead_output, q)\n",
        "        feedforward_output = self.feed_forward(residual1_output)\n",
        "        output = self.residual_layer2(feedforward_output, residual1_output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "B3SSBDwP0Xtp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Decoder"
      ],
      "metadata": {
        "id": "PhdTNYM11gBE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, dim_num=512):\n",
        "        super().__init__()\n",
        "\n",
        "        # Decoder의 구\n",
        "        self.masked_multihead = MultiHeadAttention(dim_num=dim_num)\n",
        "        self.residual_layer1 = AddLayerNorm()\n",
        "        self.multihead = MultiHeadAttention(dim_num=dim_num)\n",
        "        self.residual_layer2 = AddLayerNorm()\n",
        "        self.feed_forward = FeedForward(dim_num=dim_num)\n",
        "        self.residual_layer3 = AddLayerNorm()\n",
        "\n",
        "    def forward(self, o_q, o_k, o_v, encoder_output, mask):\n",
        "        #decoder니까 masking해야함\n",
        "        masked_multihead_output = self.masked_multihead(o_q, o_k, o_v, mask)\n",
        "        residual1_output = self.residual_layer1(masked_multihead_output, o_q)\n",
        "        # 질문은 decoder가 답은 encdoer를 통해 도출\n",
        "        multihead_output = self.multihead(residual1_output, encoder_output, encoder_output)\n",
        "        residual2_output = self.residual_layer2(multihead_output, residual1_output)\n",
        "\n",
        "        feedforward_output = self.feed_forward(residual2_output)\n",
        "        output = self.residual_layer3(feedforward_output, residual2_output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "qxxWGEEo1fEa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Transformer"
      ],
      "metadata": {
        "id": "dYvl4yx74A_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 이전에 정의한 Encoder, Decoder 등의 클래스가 있다고 가정합니다.\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder_num=6, decoder_num=6, hidden_dim=512,\n",
        "                 src_vocab_size=10000, tgt_vocab_size=10000,\n",
        "                 max_seq_length=100, dropout_ratio=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # 어휘 사전을 입력으로 받음\n",
        "        self.input_data_embed = nn.Embedding(src_vocab_size, self.hidden_dim)\n",
        "        self.output_data_embed = nn.Embedding(tgt_vocab_size, self.hidden_dim)\n",
        "\n",
        "        # 위치 인코딩은 한번만 생성\n",
        "        self.pos_encoding = self.position_encoding(max_seq_length, self.hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "        # 일반 리스트 말고 nn.ModuleList 사용해야\n",
        "        self.Encoders = nn.ModuleList([Encoder(dim_num=hidden_dim) for _ in range(encoder_num)])\n",
        "        self.Decoders = nn.ModuleList([Decoder(dim_num=hidden_dim) for _ in range(decoder_num)])\n",
        "\n",
        "        # 출력 차원을 어휘 사전 크기로 설정\n",
        "        self.last_linear_layer = nn.Linear(self.hidden_dim, tgt_vocab_size)\n",
        "\n",
        "    def position_encoding(self, max_seq_length, hidden_dim):\n",
        "        pe = torch.zeros(max_seq_length, hidden_dim) # 최대 문장 길이, 임베딩 차원으로 빈 행렬 만들기\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1) # 위치 번호를 담은 벡터\n",
        "        div_term = torch.exp(torch.arange(0, hidden_dim, 2).float() * (-math.log(10000.0) / hidden_dim )) # 차원마다 다른 주기의 파동을 만듦\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) #짝수는 사인\n",
        "        pe[:, 1::2] = torch.cos(position * div_term) #홀수는 코사인\n",
        "\n",
        "        pe = pe.unsqueeze(0) # 배치 차원 추가\n",
        "        # register_buffer: 학습되지는 않지만 모델의 state로 저장되는 텐서\n",
        "        self.register_buffer('pe', pe)\n",
        "        return pe\n",
        "\n",
        "    def forward(self, src, tgt, src_padding_mask, tgt_look_ahead_mask):\n",
        "        # 인코더 과정\n",
        "        # 임베딩 + 위치 인코딩\n",
        "        # 단어를 벡터로 변환 후 위치 정보 더해줌\n",
        "        src_embed = self.dropout(self.input_data_embed(src) + self.pe[:, :src.size(1), :])\n",
        "\n",
        "        # 여러 개의 인코더 층을 순서대로 통과\n",
        "        encoder_output = src_embed\n",
        "        for encoder in self.Encoders:\n",
        "            encoder_output = encoder(encoder_output, encoder_output, encoder_output)\n",
        "\n",
        "        # 디코더 과정\n",
        "        # 타겟 문장을 임베딩하고 위치 정보를 더함\n",
        "        tgt_embed = self.dropout(self.output_data_embed(tgt) + self.pe[:, :tgt.size(1), :])\n",
        "\n",
        "        # 여러 개의 디코더 층을 순서대로 통과\n",
        "        decoder_output = tgt_embed\n",
        "        for decoder in self.Decoders:\n",
        "            decoder_output = decoder(decoder_output, decoder_output, decoder_output,\n",
        "                                     encoder_output, tgt_look_ahead_mask)\n",
        "\n",
        "        #최종 출력 과정\n",
        "        #최종 단어 예측\n",
        "        output = self.last_linear_layer(decoder_output)\n",
        "\n",
        "        # Softmax는 보통 CrossEntropyLoss에 포함되어 있어 생략 가능\n",
        "        return output"
      ],
      "metadata": {
        "id": "9-wcLBfs4DK3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SNhwdNmQ6OGN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}